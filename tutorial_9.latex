\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{float}
\usepackage{bbm}
\graphicspath{ {./images/} }
\usepackage[bottom=0.5cm, right=1.5cm, left=1.5cm, top=1.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}

\title{Tutorial 9}
\author{Gidon Rosalki}
\date{2024-12-31}


\begin{document}
\maketitle
\section{Complete expectation formula}\label{sec:Complete expectation formula} % (fold)
\begin{definition}[]
    Let there be $X$ a uniform random variable on the probability space $\left(\Omega, \F, \p\right)$,
    and let there be $A \in \F$ an event such that $\p \left(A\right) > 0$. The
    \textbf{expectation} of $X$ given $A$ is \[
        \E \left(X | A\right) = \displaystyle\sum_{\omega \in \Omega}^{} X \left(\omega\right) \p \left(\left\{\omega\right\} | A\right) = \displaystyle\sum_{x \in Supp \left(X\right)}^{} x \p_X \left(X | A\right)
    \]
\end{definition}

\begin{theorem}[The complete expectation formula]
    Let there be $X$, a random variable with probability and $A = \left\{A_1, A_2, \dots\right\}$
    a division of the probability space, then \[
        \E \left[X\right] = \displaystyle\sum_{n \in \N}^{}\E \left[X | A_n\right] \p \left(A_n\right)
    \]
\end{theorem}

\begin{exercise}
    (From homework) We throw a die until we get the number $6$, we will say that $X$
    is the random variable which returns the number of throws. What is the expected value
    of $X$ if we know that we only threw even numbers in all the throws?

    \begin{proof}[Solution]
        We will use $A$ to be the event that we only threw even numbers. So
        $A = \left\{\left[2, 4\right]^{n - 1} \times \left\{6\right\} : n \in \N\right\}$. We want
        to find $\E \left[X | A\right]$, first we will find its distribution,
        which is of course supported by the natural numbers. Let there be $k \in \N$,
        so \begin{align*}
            \p \left(X = k | A\right) &= \displaystyle\frac{\p \left(X = k, A\right)}{\p \left(A\right)} \\
                                      &= \displaystyle\frac{\left(\displaystyle\frac{2}{6}\right)^{k - 1} \cdot \displaystyle\frac{1}{6}}{\displaystyle\sum_{n = 1}^{\infty}\left(\displaystyle\frac{2}{6}\right)^{n - 1} \cdot \displaystyle\frac{1}{6}} \\
                                      &= \displaystyle\frac{\left(\displaystyle\frac{1}{3}\right)^{k - 1}}{\displaystyle\frac{1}{1 - \displaystyle\frac{1}{3}}} \\
                                      &= \left(\displaystyle\frac{1}{3}\right)^{k - 1} \cdot \displaystyle\frac{2}{3} \\
        \end{align*}
        So we have received that $X | A \sim Geo \left(\displaystyle\frac{2}{3}\right)$,
        and therefore $\E \left[X | A\right] = \displaystyle\frac{1}{\displaystyle\frac{2}{3}} = \displaystyle\frac{3}{2}$.  \\
    \end{proof}

    Let us now throw a fair die of 3 faces, $2, 4, 6$, which we throw until we get
    $6$. We will use $Y$ to be the random variable of how many throws we make.
    Is it true that $Y \stackrel{d}{=} \left(X | A\right)$?
    \begin{proof}[Solution]
        Nope! $Y \sim Geo \left(\displaystyle\frac{1}{3}\right)$
    \end{proof}
\end{exercise}

\begin{exercise}
    We throw a die again and again, and we stop the first time we get a $1$. What
    is the expected sum of the throws thrown?

    \begin{proof}[Solution]
        Let there be $X \sim Geo \left(\displaystyle\frac{1}{6}\right)$, a random variable
        of the number of throws until a $1$ was thrown. For every $i \in \N$ we
        will define $Y_i \sim Unif \left(\left[6\right]\right)$, which will be
        a random variable whose result will be the value of the $i$th throw.
        We will define $Y$ to be the sum of the throws until a $1$ was thrown.
        From the complete expectation formula: \begin{align*}
            \E \left[Y\right] &= \displaystyle\sum_{n = 1}^{\infty} \E \left[Y | X = n\right] \p \left(X = n\right) \\
                              &= \displaystyle\sum_{n = 1}^{\infty} \E \left[\displaystyle\sum_{i = 1}^{n}Y_i | X = n\right] \cdot \left(\displaystyle\frac{5}{6}\right)^{n - 1} \cdot \displaystyle\frac{1}{6} \\
                              &= \displaystyle\sum_{n = 1}^{\infty} \displaystyle\sum_{i = 1}^{n} \E \left[Y_i | X = n\right]\left(\displaystyle\frac{5}{6}\right)^{n - 1} \displaystyle\frac{1}{6} \\
        \end{align*}
        When $i \in \left[n - 1\right]$, then $Y_i \sim Unif \left(\left[6\right] \setminus \left\{1\right\}\right)$,
        and therefore $\E \left[Y_i | X = n\right] = \displaystyle\frac{2 + 6}{2} = 4$. If
        $\left(Y_n | X = n\right) = 1$ (constant function), therefore its expected
        value is $1$. So: \begin{align*}
            \E \left[Y\right] &= \displaystyle\sum_{n = 1}^{\infty} \left(\left(n - 1\right)4 + 1\right)\left(\displaystyle\frac{5}{6}\right)^{n - 1} \displaystyle\frac{1}{6} \\
                              &= \displaystyle\sum_{n = 1}^{\infty}\left(4n - 3\right)\left(\displaystyle\frac{5}{6}\right)^{n - 1} \displaystyle\frac{1}{6} \\
                              &= 4 \displaystyle\sum_{n = 1}^{\infty} n \left(\displaystyle\frac{5}{6}\right)^{n - 1} \displaystyle\frac{1}{6} - \displaystyle\frac{1}{2} \displaystyle\sum_{n = 1}^{\infty} \left(\displaystyle\frac{5}{6}\right)^{n - 1} \\
                              &= 4 \cdot 6 - \displaystyle\frac{1}{2} \cdot \displaystyle\frac{1}{1 - \displaystyle\frac{5}{6}} \\
                              &= 21
        \end{align*}
    \end{proof}
\end{exercise}
% section Complete expectation formula (end)

\section{Markov inequality}\label{sec:Markov inequality} % (fold)
\textbf{Appears on exams!} \\
\begin{theorem}[Markov inequality]
    For $X$ uniform random variable which accepts \textbf{non negative values}, and
    has a finite expectation, then for every $c > 0$, \[
        \p \left(X \geq c\right) \leq \displaystyle\frac{\E \left[X\right]}{c}
    \]
\end{theorem}

\begin{exercise}
    A child receives some money from his parents every morning in order to buy a snack.
    In every snack there is a toy from $n$ different types. We will calculate the
    time it will take for the child to collect all the toys.

    \begin{proof}[Solution]
        For $i \in \left[n\right]$, let there be $X_i$ a random variable whose result is
        the number of days that it takes the child to get a new toy after getting
        $i - 1$ different toys. It is true that \[
            X_i \sim Geo \left(\displaystyle\frac{n - \left(i - 1\right)}{n}\right)
        \]
        The time that it will take is $X = \displaystyle\sum_{i = 1}^{n}X_i$. \begin{align*}
            \E \left[X\right] &= \displaystyle\sum_{i = 1}^{n} \E \left[X_i\right] \\
                              &= \displaystyle\sum_{i = 1}^{n} \displaystyle\frac{n}{n - i + 1} \\
                              &\stackrel{j = n - \left(i - 1\right)}{=} \displaystyle\sum_{j = 1}^{n} n \cdot \displaystyle\frac{1}{j} \\
                              &= n \displaystyle\sum_{j = 1}^{n}\displaystyle\frac{1}{j} \\
                              &= n \left(\log \left(n\right) + 1\right) \\
        \end{align*}
    \end{proof}

    After how many days will we be able to say that the probability that the
    child has not collected all the toys will be at most $\displaystyle\frac{1}{2}$?
    \begin{proof}[Solution]
        \begin{align*}
            t &\in \N \\
            \p \left(X \geq t\right) &\leq \displaystyle\frac{1}{2} \\
            \p \left(X \geq t\right) &\leq \displaystyle\frac{\E \left[X\right]}{t} \\
                                     &\leq \displaystyle\frac{n \left(\log \left(n\right) + 1\right)}{t} \\
                                     &\leq \displaystyle\frac{1}{2}
        \end{align*}
    \end{proof}
\end{exercise}

\begin{exercise}
    Let there be $n \in \N$, and $0 < p < 1$. We will define a random graph, $G \left(n, p\right)$
    (Erdos-Reyni). Two points: \begin{itemize}
        \item Let there be $X$ a random variable, whose result is the number of edges in the graph.
            So`$Supp \left(X\right) = \left\{0, 1, \dots, \binom{n}{2} \right\}$,
            and therefore $X \sim Bin \left(\binom{n}{2}, p\right)$, and $\E \left[X\right] = \binom{n}{2} p$
        \item For every node $i = 1, \dots, n$ let $D_i$ be a random variable whose result is the rank of the node
            in the graph. So $Supp \left(D_i\right) = \left\{0, \dots, n - 1\right\}$,
            and $D_i \sim Bin \left(n - 1, p\right)$
    \end{itemize}
    We will use the Markov inequality in order to find the probability that there exists
    a triangle in $G \left(n, p\right)$. Every $i \ne j \ne k \in \left\{1, \dots, n\right\}$
    represent a triangle with probability $p^3$. We will define $X_{i, j, k} \sim Ber \left(p^3\right)$
    to be the number of triangles in the graph, represented by the
    random variable $T = \displaystyle\sum_{i, j, k}^{}X_{i, j, k}$. From Markov's inequality
    we will get \begin{align*}
        \p \left(T \geq 1\right) &\leq \displaystyle\frac{\E \left[\displaystyle\sum_{i, j, k}^{} X_{i, j, k}\right]}{1}
                                 &= \displaystyle\sum_{i, j, k}^{}\E \left[X_{i, j,k}\right] \\
                                 &= \displaystyle\sum_{i, j, k}^{}p^3 \\
                                 &= \binom{n}{3} p^3
    \end{align*}

\end{exercise}
% section Markov inequality (end)

\section{Variance}\label{sec:Variance} % (fold)
\begin{definition}[Variance]
    For a discrete random variable $X$ such that $\E \left[X^2\right] < \infty$,
    the \textbf{variance} is \[
        Var \left[X\right] = \E \left[X - \E \left[X\right]^2\right] = \E \left[X^2\right] - \E \left[X\right]^2
    \]
    For every $c \in \R$ it is true that: \begin{enumerate}
        \item $Var \left[X\right] \geq 0$ and $Var \left[X\right] = 0 \Leftrightarrow X$ is constant
        \item $Var \left[X + c\right] = Var \left[X\right]$ \\
        \item $Var \left[cX\right] = c^2 Var \left[X\right]$
        \item If $X, Y$ are independent random variables with finite variance,
            then $Var \left[X + Y\right] = Var \left[X\right] + Var \left[Y\right]$
    \end{enumerate}
\end{definition}

\subsubsection{Variance of a Bernoulli random variable}\label{sec:Variance of a Bernoulli random variable} % (fold)
\begin{align*}
    X &\sim Ber \left(p\right) \\
    Var \left[X\right] &= \E \left[X^2\right] - \E \left[X\right]^2 \\
                       &= p - p^2 \\
                       &= p \left(1 - p\right) \\
\end{align*}
% subsubsection Variance of a Bernoulli random variable (end)

\subsubsection{Variance of a uniform random variable}\label{sec:Variance of a uniform random variable} % (fold)
\begin{align*}
    X &\sim Unif \left(\left[n\right]\right) \\
    Var \left[X\right] &= \displaystyle\sum_{k \in \left[n\right]}^{} k^2 \p \left(X = k\right) - \left(\displaystyle\frac{n + 1}{2}\right)^2 \\
                       &= \displaystyle\frac{n \left(n + 1\right) \left(2n + 1\right)}{6} - \left(\displaystyle\frac{n + 1}{2}\right)^2 \\
                       &= \displaystyle\frac{n^2 -1}{12}
\end{align*}
% subsubsection Variance of a uniform random variable (end)

\subsubsection{Variance of a binomial random variable}\label{sec:Variance of a binomial random variable} % (fold)
\begin{align*}
    X &\sim Bin \left(n, p\right) \\
    X &\stackrel{d}{=} \displaystyle\sum_{k \in \left[n\right]}^{}Y_k : Y_k \sim Ber \left(p\right) \\
    Var \left[X\right] &= Var \left[\displaystyle\sum_{k \in \left[n\right]}^{}Y_k\right]
      &= \displaystyle\sum_{k \in \left[n\right]}^{} Var \left[Y_k\right] \\
                       &= np \left(1 - p\right)
\end{align*}
% subsubsection Variance of a binomial random variable (end)
% section Variance (end)

\end{document}
