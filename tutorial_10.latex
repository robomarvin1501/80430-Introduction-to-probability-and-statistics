\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{float}
\usepackage{bbm}
\graphicspath{ {./images/} }
\usepackage[bottom=0.5cm, right=1.5cm, left=1.5cm, top=1.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}

\title{Tutorial 10}
\author{Gidon Rosalki}
\date{2025-01-07}


\begin{document}
\maketitle
\section{Variance}\label{sec:Variance} % (fold)
\begin{definition}[Variance]
    For a discrete random variable $X$ where $\E \left[X^2\right] < \infty$, its
    \textbf{variance} is defined as \[
        Var \left[X\right] = \E \left[\left(X - \E \left[X\right]\right)^2\right] = \E \left[X^2\right] - \E \left[X\right]^2
    \]
\end{definition}

\begin{exercise}
    In a class of $n$ students, given the assumption that their birthdays is distributed uniformly
    over a given year, and that they are independent, what is the expectation
    and variance of the number of pairs of students that share a birthday?

    \begin{proof}[Solution]
        Let there be $X$ a random variable that counts the number of pairs
        that have the same birthday. We will define $Y_i \sim Unif \left(1, \dots, 365\right)$,
        random variables that return the birthday of the child $i$. For every
        $i, j \in \left[n\right]$, let there be $X_{i, j} \sim Ber \left(\displaystyle\frac{1}{365}\right)$,
        random variable that checks if a pair of children $i, j$ share a birthday.
        So $X = \displaystyle\sum_{1 \leq i < j \leq n}^{}X_{i, j}$. We will calculate
        the expectation: \begin{align*}
            \E \left[X\right] &= \E \left[\displaystyle\sum_{1 \leq i < j \leq n}^{}X_{i, j}\right] \\
                              &= \displaystyle\sum_{1 \leq i < j \leq n}^{}\E \left[X_{i, j}\right] \\
                              &= \binom{n}{2} \cdot \displaystyle\frac{1}{365} \\
        \end{align*}
        Let us find the variance: We shall being by asking if \begin{align*}
            \p \left(X_{i, j} = 1, X_{m, n} = 1\right) &= \p \left(X_{i, j} = 1\right) \p \left(X_{m, n} = 1\right)
        \end{align*}
        or in words, are they independent? \\
        \begin{align*}
            \p \left(X_{i, j} = 1, X_{m, n} = 1\right) &= \p \left(Y_j = Y_i, Y_m = Y_n\right) \\
                                                       &= \begin{cases}
                                                        \p \left(Y_j = Y_i\right) \p \left(Y_n = Y_m\right), &\text{ if }\left\{j, i\right\} \cap \left\{n, m\right\} = \emptyset\\
                                                        \p \left(Y_j = Y_i = Y_n\right), &\text{ if } m \in \left\{j, i\right\}
                                                       \end{cases} \\
                                                       &= \begin{cases}
                                                        \left(\displaystyle\frac{1}{365}\right)^2, &\text{ if }\left\{j, i\right\} \cap \left\{m, n\right\} = \emptyset\\
                                                        \p \displaystyle\sum_{k = 1}^{365}\p \left(Y_j = Y_i = Y_n = k\right), &\text{ if } m \in \left\{i, j\right\}
                                                       \end{cases} \\
                                                       &= \begin{cases}
                                                        \left(\displaystyle\frac{1}{365}\right)^2, &\text{ if }\left\{j, i\right\} \cap \left\{m, n\right\} = \emptyset\\
                                                        \p \displaystyle\sum_{k = 1}^{365}\p \left(Y_j = k\right) \p \left(Y_i = k\right) \p \left(Y_n = k\right), &\text{ if } m \in \left\{i, j\right\}
                                                       \end{cases} \\
                                                       &= \begin{cases}
                                                        \left(\displaystyle\frac{1}{365}\right)^2, &\text{ if }\left\{j, i\right\} \cap \left\{m, n\right\} = \emptyset\\
                                                        \p \left(\displaystyle\frac{1}{365}\right)^2, &\text{ if } m \in \left\{i, j\right\}
                                                       \end{cases}
        \end{align*}
        So we have shown that they are in fact independent.
        \begin{align*}
            Var \left[X\right] &= Var \left[\displaystyle\sum_{1 \leq i < j \leq n}^{}X_{i, j}\right] \\
                               &= \displaystyle\sum_{1 \leq i < j \leq n}^{}Var \left[X_{i, j}\right] \\
                               &= \binom{n}{2} \displaystyle\frac{1}{365} \left(1 - \displaystyle\frac{1}{365}\right) \\
        \end{align*}
    \end{proof}
\end{exercise}
% section Variance (end)

\section{Chebishev inequality}\label{sec:Chebishev inequality} % (fold)
\begin{definition}[Chebishev inequality]
    Let there be a discrete random variable $X$, with expectation, and finite variance,
    then for all $c > 0$ \[
        \p \left(\left|X - \E \left[X\right]\right| \geq c\right) \leq \displaystyle\frac{Var \left[X\right]}{c^2}
    \]
\end{definition}
If $c$ enables $c - \E \left[X\right] > 0$ then \begin{align*}
    \p \left(X \geq c\right) &= \p \left(X - \E \left[X\right] > c - \E \left[X\right]\right) \\
                             &\leq \p \left(\left|X - \E \left[X\right]\right| \geq c - \E \left[X\right]\right) \\
                             &\leq \displaystyle\frac{Var \left[X\right]}{\left(c - \E \left[X\right]\right)^2}
\end{align*}

\begin{example}[]
    Alice arrived at the foyer, in which there are two doors. With a probability
    of $p$, Alice will choose the door that will return her to wonderland, and
    with a chance of $1 - p$ the door that returns her to the foyer. Find the limit
    of the probability that Alice will arrive at wonderland after at least 10 attempts.

    \begin{proof}[Solution]
        Let there be a random variable $X$ that counts how many attempts Alice took
        to return to wonderland. $X \sim Geo \left(p\right)$. From the
        Chebishev inequality: \begin{align*}
            \p \left(X \geq 10\right) &\leq \displaystyle\frac{Var \left[X\right]}{\left(c - \E \left[X\right]\right)^2} \\
                                      &= \displaystyle\frac{\left(\displaystyle\frac{1 - p}{p^2}\right)}{\left(10 - \displaystyle\frac{1}{p}\right)^2} \\
                                      &= \displaystyle\frac{1 - p}{\left(10p - 1\right)^2} \\
        \end{align*}
    \end{proof}
\end{example}
% section Chebishev inequality (end)

\section{Covariance}\label{sec:Covariance} % (fold)
\begin{definition}[Covariance]
    For the discrete random variables $X, Y$ with finite variance, the
    covariance is: \[
        Cov \left[X, Y\right] = \E \left[\left(X - \E \left[X\right]\right)\left(Y - \E \left[Y\right]\right)\right] = \E \left[XY\right] - \E \left[X\right] \E \left[Y\right]
    \]
\end{definition}
Features:
\begin{enumerate}
    \item For every random variable $X$, $Cov \left[X, X\right] = Var \left[X\right]$
    \item Symmetry: $Cov \left[X, Y\right] = Cov \left[Y, X\right]$
    \item Linearity: $Cov \left[aX + bY, Z\right] = a Cov \left[X, Z\right] + b Cov \left[Y, Z\right]$
    \item Apathy to movement: $Cov \left[X + a, Y\right] = Cov \left[X, Y\right]$
    \item If $X, Y$ are random variables, with finite variance, then \[
            Var \left[X + Y\right] = Var \left[X\right] + Var \left[Y\right] + 2 Cov \left[X, Y\right]
        \]
        Or in general: \begin{align*}
            Var \left[\displaystyle\sum_{i = 1}^{n}X_i\right] &= \displaystyle\sum_{i = 1}^{n}Var \left[X_i\right] + 2 \displaystyle\sum_{1 \leq i < j \leq n}^{}Cov \left[X_i, X_j\right]\\
        \end{align*}
\end{enumerate}

\begin{example}[]
    We throw 2 fair dice, and we want to find the covariance
    between the sum of the throws and the result of one of them.
    \begin{proof}[Solution]
        $X, Y \sim Unif \left[6\right]$. We are looking for $Cov \left[X, X + Y\right]$.
        \begin{align*}
            Cov \left[X, X + Y\right] &= Cov \left[X, X\right] + Cov \left[X, Y\right] \\
                                      &= Var \left[X\right] + Cov \left[X, Y\right] \\
                                      &= Var \left[X\right] + 0 \\
                                      &= \displaystyle\frac{6^2 - 1}{12}
        \end{align*}
    \end{proof}
\end{example}

\begin{exercise}
    We sort the numbers $\left[n\right]$ in a line, in a random order, uniformly.
    Let there be $X$, a random variable, whose result is the number of variables
    $k \in \left[n\right]$ are located in the correct position in the line.

    \begin{proof}[Solution]
        Variance of $X$: For $i \in \left[n\right]$, let there be $X_i \sim Ber \left(p\right)$,
        random variables whose results are $1$ \textbf{if and only if} the number $i$
        is located in the correct position. The number of possible orderings is $n!$,
        and the number of ordering where $i$ is located in its position is $\left(n - 1\right)!$.
        Therefore $p = \displaystyle\frac{1}{n}$. It is true that
        $X = \displaystyle\sum_{i = 1}^{n}X_i$, and so \begin{align*}
            \E \left[X\right] &= \E \left[\displaystyle\sum_{i = 1}^{n}X_i\right] \\
                              &= \displaystyle\sum_{i = 1}^{n}\E \left[X_i\right] \\
                              &= n \cdot \left(\displaystyle\frac{1}{n}\right) \\
                              &= 1
        \end{align*}
        To find the variance is trickier, since the variables $X_i, X_j$ are dependent.
        \[
            \p \left(X_1 = 1, X_2 = 1\right) = \displaystyle\frac{\left(n - 2\right)!}{n!} = \displaystyle\frac{1}{\left(n - 1\right)n} \ne \displaystyle\frac{1}{n^2} = \p \left(X_1 = 1\right) \p \left(X_2 = 1\right)
        \]
        \begin{align*}
            Var \left[X\right] &= Var \left[\displaystyle\sum_{i = 1}^{n}X_i\right] \\
                               &= \displaystyle\sum_{i = 1}^{n}Var \left[X_i\right] + 2 \displaystyle\sum_{1 \leq i < j \leq n}^{}Cov \left[X_i, X_j\right] \\
                               &= n \cdot \displaystyle\frac{1}{n} \left(1 - \displaystyle\frac{1}{n}\right) + 2 \binom{n}{2} \displaystyle\frac{1}{n^2 \left(n - 1\right)}\\
                               &= \displaystyle\frac{n - 1}{n} + 2 \binom{n}{2} \displaystyle\frac{1}{n^2 \left(n - 1\right)}\\
                               &= 1 \\
        \end{align*}
        For some $1 \leq k \leq n$ we will limit the probability that at least $k$
        variable were put in their places. From the Markov inequality: \[
            \p \left(X \geq k\right) \leq \displaystyle\frac{\E \left[X\right]}{k} = \displaystyle\frac{1}{k}
        \]
        For $k > 1$ we can get a better limit by using the Chebishev inequality: \begin{align*}
            \p \left(X \geq k\right) &= \p \left(X - \E \left[X\right] \geq k - 1\right) \\
                                     &\leq \p \left(\left|X - \E \left[X\right]\right| \geq k - 1\right) \\
                                     &\leq \displaystyle\frac{Var \left[X\right]}{\left(k - 1\right)^2} \\
                                     &= \displaystyle\frac{1}{\left(k - 1\right)^2} \\
        \end{align*}
    \end{proof}
\end{exercise}

% section Covariance (end)

\section{Notes and definitions}\label{sec:Notes and definitions} % (fold)
\begin{definition}[Uncoordinated]
    If $X, Y$ are random variables such that \[
        Cov \left[X, Y\right] = 0
    \]
    then they are called "uncoordinated"
\end{definition}
\begin{theorem}[]
    $X, Y$ are independent $\implies$ uncoordinated
\end{theorem}
% section Notes and definitions (end)

\section{Weak rule of large numbers}\label{sec:Weak rule of large numbers} % (fold)
\begin{theorem}[]
    $X$ is a random variable with finite variance, let $X_1, \dots, $ be independent random variables
    of equal distribution to $X$. Then for all $\varepsilon > 0$ \[
        \displaystyle\lim_{n \to \infty}\p \left(\left|\displaystyle\frac{1}{n}\displaystyle\sum_{i = 1}^{n}X_i - \E \left[X\right]\right| < \varepsilon\right) = 1
    \]
\end{theorem}
% section Weak rule of large numbers (end)
\end{document}
