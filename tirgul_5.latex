\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{float}
\graphicspath{ {./images/} }
\usepackage[bottom=0.5cm, right=1.5cm, left=1.5cm, top=1.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}

\title{Tutorial 8}
\author{Gidon Rosalki}
\date{2024-11-26}


\begin{document}
\maketitle
\section{Independent events}\label{sec:Independent events} % (fold)
\begin{definition}[Independent events]
    Let there be $\left(\Omega, \F, \p\right)$, the two events $A, B \in \F$ are
    \textbf{independent} if $\p \left(A \cap B\right) = \p \left(A\right) \p \left(B\right)$

    \textbf{NOTE: } Independence of 2 events does \textbf{not} imply that they are
    disjoint, and in fact the reverse! If the two events $A, B$ are independent,
    then they cannot be disjoint, since $\p \left(A \cap B\right) > 0$
\end{definition}

\begin{exercise}
    We take a random card from a deck of 52 cards. Let $E$ be the event of getting
    an ace, and $F$ be the event of getting a heart. Are these events independent?

    \begin{proof}[Solution]
        $\Omega = $ the cards, $|\Omega| = 52$. There are 4 aces, so $\p \left(E\right) = \displaystyle\frac{4}{52}$,
        and 13 hearts, so $\p \left(F\right) = \displaystyle\frac{13}{52}$. \\
        There is only 1 Ace of Hearts, so
        \[
            \displaystyle\frac{1}{52} = \p \left(E \cap F\right) = \p \left(E\right) \p \left(F\right) = \displaystyle\frac{13 \cdot 4}{52 \cdot 52}
        \]
        so yes they are independent events.
    \end{proof}
\end{exercise}

\begin{exercise}
    There are 3 coins in a bag, 2 fair, and one double headed. We pick one coin,
    and throw it twice. For $i = 1, 2$, we will write $A_i$ to be the event where
    we throw heads on the $i$th throw.

    \begin{proof}[Solution]
        $\Omega$ is all the possible choices of coins. $D$ is the event that we have taken a fair
        coin from the jug $\p \left(D\right) = \displaystyle\frac{2}{3}$. \\
        $\p \left(A_1 | D\right) = \displaystyle\frac{1}{2} \land \p \left(A_1 | D^c\right) = 1$.
        So according to the complete probability formula: \begin{align*}
            \p \left(A_1\right) &= \p \left(D\right) \cdot \p \left(A_1 | D\right) + \p \left(D^c\right) \cdot \p \left(A_1 | D^c\right) \\
                                &= \displaystyle\frac{2}{3} \cdot \displaystyle\frac{1}{2} + \displaystyle\frac{1}{3} \cdot 1 \\
                                &= \displaystyle\frac{2}{3}
        \end{align*}
        From the same calculation we can see that $\p \left(A_2\right) = \displaystyle\frac{2}{3}$.
        On the other hand:
        \begin{align*}
            \p \left(A_1 \cap A_2\right) &= \p \left(A_1 \cap A_2 | D\right) \p \left(D\right) + \p \left(A_1 \cap A_2 | D^c\right) \p \left(D^c\right) \\
                                         &= \displaystyle\frac{1}{2} \cdot \displaystyle\frac{1}{2} \cdot \displaystyle\frac{2}{3} +
                                         1 \cdot 1 \cdot \displaystyle\frac{1}{3} \\
                                         &= \displaystyle\frac{1}{6} + \displaystyle\frac{1}{3} \\
                                         &= \displaystyle\frac{1}{2}
        \end{align*}
        However $\p \left(A_1\right) \cdot \p \left(A_2\right) = \displaystyle\frac{2}{3} \cdot \displaystyle\frac{2}{3} = \displaystyle\frac{4}{9} \ne \displaystyle\frac{1}{2}$,
        so the events are not independent.
    \end{proof}
\end{exercise}

\begin{exercise}
    Three judges are deciding the fate of an accused by majority decision. 2 of
    the judges are experienced, and correctly judge the guilt of an accused with
    a probability of $90\%$. The third is inexperienced, and judges the guilt with
    a probability of $51\%$. Find the probability of a correct judgement in the
    following cases:
    \begin{enumerate}
        \item The judgement of each judge is independent
        \item The two experienced judges choose independently, and the inexperienced
            chooses one of the opinions of the experienced judges randomly.
    \end{enumerate}

    \begin{proof}[Solution]
        \begin{enumerate}
            \item We will use $A$ to be the event where the judgement is correct, and $A_i$
                is the event that the $i$th judge chose correctly, where for $i = 3$ is
                the inexperienced judge. $A$ will occur if at least 2 of $\left\{A_1, A_2, A_3\right\}$
                occur.
                \[
                    A = \left(A_1 \cap A_2\right) \cup \left(A_1^c \cap A_2 \cap A_3\right) \cup \left(A_1 \cap A_2^c \cap A_3\right)
                \]
                and these are disjoint events.
                \begin{enumerate}
                    \item $\p \left(A_1 \cap A_2\right) = \p \left(A_1\right) \cdot \p \left(A_2\right) = 0.9^2$
                    \item $\p \left(A_1^c \cap A_2 \cap A_3\right) = \p \left(A_1 \cap A_2^c \cap A_3\right) = \p \left(A_1^c\right) \p \left(A_2\right) \p \left(A_3\right) = 0.1 \cdot 0.9 \cdot 0.51$
                \end{enumerate}
                In total, $\p \left(A\right) = 0.9^2 + 2 \cdot 0.1 \cdot 0.9 \cdot 0.51 = 0.9018$
            \item \begin{align*}
                    \p \left(A_1 \cap A_2\right) &= \p \left(A_1\right) \cdot \p \left(A_2\right) = 0.9^2 \\
                    \p \left(A_1^c \cap A_2 \cap A_3\right) &= \p \left(A_1 \cap A_2^c \cap A_3\right) = \p \left(A_1 \cap A_2^c\right) \cdot \p \left(A_3 | A_1 \cap A_2^c\right)
                    = 0.9 \cdot 0.1 \cdot 0.5
            \end{align*}
            In total:
            \[
                \p \left(A\right) = 0.9^2 + 2 \cdot 0.9 \cdot 0.1 \cdot 0.5 = 0.9
            \]
        \end{enumerate}
        So it is preferable that the inexperienced judge will choose independently of
        the others. The difference is so small because the inexperienced judge is
        so close to the probability of just flipping a coin. If he were more experienced,
        then the difference would increase.
    \end{proof}
\end{exercise}

\subsubsection{Independence of a number of events}\label{sec:Independence of a number of events} % (fold)
A collection of events $A_1, \dots, A_n$ is called a collection of independent
events if for every sub collection $A_1, \dots, A_k$ it is true that $\p \left(A_1 \cap \dots \cap A_k\right) = \p \left(A_1\right) \cdot \dots \cdot \p \left(A_k\right)$.
Independence in pairs is weaker than independence.

% TODO additional exercises
% subsubsection Independence of a number of events (end)
% section Independent events (end)

\section{Random variables}\label{sec:Random variables} % (fold)
$X : \Omega \to \R$

\subsubsection{Examples}\label{sec:Examples} % (fold)
\begin{example}
    What is the probability that a random person in the street is taller than
    $1.70$ metres?. In the statistical model, $\Omega$ is the collection of
    all possible random people. $\p$ is the function that will define who is more
    likely to be met. For the height, we will define $X: \Omega \to \R$, such that
    \[
        X(\omega) = \text{Height of } \omega
    \]
    The question has been changed to the calculation of the probability of the
    event $\left\{X > 1.70\right\}$, which is to say \[
        \p \left(\left\{X > 1.70\right\}\right) = \p \left(\left\{\omega \in \Omega: X(\omega) > 1.70\right\}\right)
    \]
\end{example}

\begin{example}
    Suppose we throw a die twice. What is the probability that the result of the
    first throw is larger than the result of the second throw? If $\left(\Omega, \F, \p\right)$
    is the suitable probability space of the question, for $i = 1, 2$ we
    will define $X_i$ to be the value of the $i$th throw. \[
        X_i(\omega) = \text{Output of the } i\text{th roll in } \omega
    \]
    and the question has been changed to calculating $\p \left(X_1 > X_2\right)$.
    For example, if we consider $\Omega = [6]^2$ (like usual), we can precisely
    define the following: \begin{align*}
        X_1(a, b) &= a \\
        X_2(a, b) &= b
    \end{align*}
\end{example}
% subsubsection Examples (end)

\subsubsection{Distribution of random variables}\label{sec:Distribution of random variables} % (fold)
\begin{definition}[]
    Let there be $X: \Omega \to \R$, a random variable on the probability space: $\left(\Omega, \p\right)$.
    For a set $E \subseteq \R$, we will use the following contraction: \[
        \left\{X \in E\right\} = X^{-1}(E) = \left\{\omega \in \Omega : X(\omega) \in E\right\}
    \]
    The distribution of $X$ is the new probability function on the space $\R$, defined
    by \[
        \forall E \subseteq \R\ \p_X(E) = \p \left(X \in E\right)
    \]
    If $\p_X$ is a discrete probability function, then we will say that $X$ is a
    discrete random variable. If $X$ is a discrete random variable, then we will
    define the \textbf{support} of $X$ to be the countable set \[
        Supp(X) = \left\{x \in \R : p_X(x) > 0\right\}
    \]
\end{definition}

Let there be $\left(\Omega_1, \F_1, \p_1\right)$ $\left(\Omega_2, \F_2, \p_2\right)$, two probability spaces,
for the two random variables $X: \Omega_1 \to \R$ and $Y: \Omega_2 \to \R$ we will
say that they have equivalen distribution if $\p_X = \p_Y$. In this case we will write
the symbol $X \stackrel{d}{=} Y$

\begin{exercise}
    We choose a number from $[2]$ twice. We will look at the random variable where its
    output is the sum of the selected numbers. This is a random variable that takes the inputs
    $2, 3, 4$. We will show different ways to show this random variable.

    \begin{proof}[Solution]
        \begin{enumerate}
            \item $\Omega_1 = [2]^2$, with $\p_1$ a continuous probability function. In
                this model, $X: \Omega_1 \to \R$ is defined by $X(i, j) = i + j$.
                It follows that \[
                    \p_X(2) = \p_1(X = 2) = \p_1(\left\{\left(1, 1\right)\right\}) = \displaystyle\frac{1}{4}
                \]
                Similarly, $\p_X(4) = \displaystyle\frac{1}{4}$, However\[
                    \p_X(3) = \p_1 \left(X = 3\right) = \p_1 \left(\left\{\left(1, 2\right), \left(2, 1\right)\right\}\right) = \displaystyle\frac{2}{4} = \displaystyle\frac{1}{2}
                \]
            \item We will use $X_i$ to be the number of times that we chose $i$. So
                $\Omega_2 = \left\{\left(x_1, x_2\right) \in \left\{0, 1, 2\right\}^2 | x_1 + x_2 = 2\right\}$,
                with the following probability function: \[
                    \p_2 \left(0, 2\right) = \p \left(2, 0\right) = \displaystyle\frac{1}{4},\ \p_2 \left(1, 1\right) = \displaystyle\frac{1}{2}
                \]
                In this model we will define $Y: \Omega_2 \to \R$ as follows \[
                    Y(2, 0) = 2,\ Y(0, 2) = 4,\ Y(1, 1) = 3
                \]
                Now \[
                    \p_Y(2) = \p_2(Y = 2) = \p \left(2, 0\right) = \displaystyle\frac{1}{4}
                \]
                Similarly, $\p_Y(4) = \displaystyle\frac{1}{4}$, however \[
                    \p_Y(3) = \p_2(Y = 3) = \p_2(1, 1) = \displaystyle\frac{1}{2}
                \]
        \end{enumerate}
        So in conclusion $X \stackrel{d}{=} Y$
    \end{proof}
\end{exercise}

\begin{exercise}
    We throw 2 dice, with $N$ as the difference between the results. What is the
    distribution of $N$?
    \begin{proof}[Solution]
        $\Omega = [6]^2$, $\p$ uniform probability function, every element $\Omega$
        is of the type $\omega = \left(\omega_1, \omega_2\right)$. We will use
        $X_i$ to be the result of the throw of the $i$th die, and $N$ will be the
        differences.

        The solution is equvalent to finding the distribution function of $N$.
        \begin{align*}
            p_N(0) &= \p \left(N = 0\right) = \p \left(\left\{\left(n, n\right) : n \in [6]\right\}\right) = \displaystyle\frac{1}{6} \\
            p_N(1) &= \p \left(N = 1\right) = \p \left(\left\{\left(n + 1, n\right) : n \in [5]\right\}\right) = \displaystyle\frac{5}{36} \\
                   &\text{and in general} \\
            \forall k \geq 0 p_N(k) &= \p \left(\left\{\left(n + k, n\right) : n \in [6-k]\right\}\right) = \displaystyle\frac{6 - k}{36}
        \end{align*}
        and for $k < 0$ we get $p_N(k) = \displaystyle\frac{6 + k}{36}$
    \end{proof}
\end{exercise}
% subsubsection Distribution of random variables (end)
% section Random variables (end)

\section{Almost always equivalence of random variables}\label{sec:Almost always equivalence of random variables} % (fold)
\begin{definition}[]
    $\left(\Omega, \F, \p\right)$, and $X, Y: \Omega \to \R$. We will say that $X$ is equivalent almost always to $Y$ if
    \[
        \p \left(\left\{X = y\right\}\right) = \p \left(\left\{\omega \in \Omega : X(\omega) = Y(\omega)\right\}\right) = 1
    \]
    and this is written $X \stackrel{a.s}{=} Y$
\end{definition}
% section Almost always equivalence of random variables (end)
\end{document}
