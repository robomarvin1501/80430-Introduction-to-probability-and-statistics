\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{float}
\usepackage{bbm}
\graphicspath{ {./images/} }
\usepackage[bottom=0.5cm, right=1.5cm, left=1.5cm, top=1.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}

\title{Tutorial 6}
\author{Gidon Rosalki}
\date{2024-12-03}


\begin{document}
\maketitle
\section{Reminder of distributions}\label{sec:Reminder of distributions} % (fold)
Let there be $\left(\Omega, \F, \p\right)$.
\begin{definition}[Bernoulli distribution]
    We will say that the random variable $X$ defined on $\Omega$ is bernoulli
    distributed with parameter $p$ (written $X \sim Ber(p)$) if \begin{align*}
        p_X(1) &= p \\
        p_X(0) &= 1- p
    \end{align*}
\end{definition}

\begin{definition}[Uniform distribution]
    We will say that the random variable $X$ is uniformly distributed on some
    finite set $S \subset \R$ if $\p_X \left(\left\{t\right\}\right) = \displaystyle\frac{1}{|S|}$.
    In this case we write $X \sim Unif(S)$
\end{definition}
% section Reminder of distributions (end)

\section{Random vectors}\label{sec:Random vectors} % (fold)
\begin{definition}[Random vector]
    The finite collection of random variables $X = \left(X_1, \dots, X_n\right)$ defined on a
    probability space will be called a random vector. For some random vector
    $X = \left(X_1, \dots, X_n\right)$ on the probability space $\left(\Omega, \F, \p\right)$,
    the function $\p_X = \p_{X_1, \dots, X_n}: \F_{\R^n} \to [0, 1]$ defined by
    \[
        \p_X(A) = \p \left(X^{-1}(A)\right) = \p \left(\left\{\omega \in \Omega : X(\omega) \in A\right\}\right)
    \]
    is called the \textbf{shared distribution} of $X = \left(X_1, \dots, X_n\right)$.
    The distributions of each $X_1, \dots, X_n$ are called the \textbf{marginal
    distributions}.
\end{definition}

\begin{example}
    We throw 2 fair dice, and we will use $X$ to be the sum of the dice, and
    $Y$ to be the difference between them. The marginal distributions of $\left(X, Y\right)$
    are given as follows:\begin{align*}
        \forall k \in \left\{2, \dots, 12\right\}\ \p_X \left(\left\{k\right\}\right) &= \displaystyle\frac{6 - |7 - k|}{36} \\
        \forall k \in \left\{-5, \dots, 5\right\}\ \p_Y \left(\left\{k\right\}\right) &= \displaystyle\frac{6 - |k|}{36}
    \end{align*}
    The shared distribution is given as follows: \[
        \forall \left(x, y\right) \in \R^2 : \left(\displaystyle\frac{x + y}{2}, \displaystyle\frac{x - y}{2}\right) \in [6]^2
        \implies \p_{X, Y} \left(\left\{x, y\right\}\right) = \p \left(\left\{\displaystyle\frac{x + y}{2}, \displaystyle\frac{x - y}{2}\right\}\right) = \displaystyle\frac{1}{36}
    \]
    and $0$ in every other case.
\end{example}

\begin{exercise}
    We throw 3 fair dice, and use $X_i$ to be the random variable that returns the
    result of the $i$th cube. We will define the random vector $X = \left(X_1, \displaystyle\frac{X_1 + X_2}{2}, \displaystyle\frac{X_1 + X_2 + X_3}{3}\right)$.
    We will use $S \subseteq \R^3$ to be $S = \left\{\left(a, a, a\right) | a \in \R\right\}$. What is the probability of the event
    $\left\{X \in S\right\}$?

    \begin{proof}[Solution]
        The event space is $\Omega = [6]^3$. The probability function $\p$ is uniform.
        \begin{align*}
            \p_X \left(S\right) &= \p \left(\left\{\omega \in \Omega | X(\omega) \in S\right\}\right) = \p \left(\left\{\left(x_1, x_2, x_3\right) \in \Omega | x_1 = \displaystyle\frac{x_1 + x_2}{2} = \displaystyle\frac{x_1 + x_2 + x_3}{3}\right\}\right) \\
                                &= \p \left(\left\{\left(x_1, x_2, x_3\right) \in \Omega : x_1 = x_2 = x_3\right\}\right) = \p \left(\left\{\left(a, a, a\right) \in \Omega | a \in [6]\right\}\right) \\
                                &= \displaystyle\frac{6}{6^3} = \displaystyle\frac{1}{36} \\
        \end{align*}
    \end{proof}
\end{exercise}

\begin{exercise}
    \textbf{Continuation of the previous: } this time we shall use the random vector
    $Y = \left(X_1, X_1 + X_2, X_1 + X_2 + X_3\right)$. With $T = \left\{\left(a, 2a+1, 0\right) | a \in \R\right\}$,
    what is the probability that $\left\{Y \in T\right\}$?

    \begin{proof}[Solution]
        \begin{align*}
            \p_Y(T) &= \p \left(Y^{-1}(T)\right) = \p \left(\left\{\left(y_1, y_2, y_3\right) \in \Omega | y_1 + y_2 = 2y_1 + 1,\ y_1 + y_2 = y_3\right\}\right) \\
                    &= \p \left(\left\{y_1, y_2, y_3\right\} \in \Omega | y_2 = y_1 + 1,\ y_3 = 2y_1 + 1\right) \\
                    &= \p \left(\left\{\left(1, 2, 3\right), \left(2, 3, 5\right)\right\}\right) = \displaystyle\frac{2}{6^3}
        \end{align*}
    \end{proof}
\end{exercise}
% section Random vectors (end)

\section{Distribution given an event}\label{sec:Distribution given an event} % (fold)
\begin{definition}[]
    Let there be a random variable $X$ on $\left(\Omega, \F, \p\right)$, and $A \in \F$,
    such that $\p \left(A\right) > 0$. We will write $\p_A = \p \left(\left(\cdot\right) | A\right)$
    to be the conditional probability function on $\Omega$ \[
        \forall B \in \F,\ \p_A(B) = \p \left(B | A\right) = \displaystyle\frac{\p \left(B \cap A\right)}{\p \left(A\right)}
    \]
    additionally we will use $\left(X | A\right)$ to be the function $X: \Omega \to \R$,
    which is a random variable on the probability space $\left(\Omega, \F, \p \left(\left(\cdot\right) | A\right)\right)$.


    Let there be $X$, a random variable on $\left(\Omega, \F, \p\right)$, and $A \in \F$
    such that $\p \left(A\right) > 0$, the distribution of $X | A$ (also called the
    distribution of $X$ given $A$) is \[
        \forall E \subseteq \R,\ \p_{X | A} \left(E\right) = \p \left(\left\{X \in E\right\} | A\right)
    \]
\end{definition}

\begin{exercise}
    we throw 3 weighted coins, with a parameter $p$, one after the other. We will
    use the three independent random variables $X_1, X_2, X_3 \sim Ber(p)$. How
    does the random vector $X = \left(X_1, X_2, X_3\right)$ distribute if
    it is known that $X_1 + X_2 + X_3 = 2$

    \begin{proof}[Solution]
        \begin{align*}
            A &= \left\{\left(1, 1, 1\right), \left(1, 0, 1\right), \left(0, 1, 1\right)\right\} \\
            \p \left(A\right) &= 3p^2 \left(1 - p\right) \\
            \p_{X | A} \left(X = \left(x_1, x_2, x_3\right)\right) &= \p \left(X = \left(x_1, x_2, x_3\right) | A\right) \\
                                                                   &= \p_A \left(X = \left(x_1, x_2, x_3\right)\right) \\
                                                                   &= \displaystyle\frac{\p \left(\left(x_1, x_2, x_3\right) \cap A\right)}{\p \left(A\right)} \\
                                                                   &= \displaystyle\frac{\p \left(x_1, x_2, x_3\right)}{\p \left(A\right)} \\
                                                                   &= \displaystyle\frac{p^2 \left(1 - p\right)}{3p^2 \left(1 - p\right)} \\
                                                                   &= \displaystyle\frac{1}{3}
        \end{align*}
    \end{proof}
\end{exercise}

\begin{exercise}
    We throw a fair coin. If we get $0$, then we once again throw a fair coin,
    and if we get $1$, then we throw a weighted coin with probability $p$ of getting
    $1$. What is the distribution of the second throw?

    \begin{proof}[Solution]
        Let there be $X$, the result of the first throw. So $X \sim Ber \left(\displaystyle\frac{1}{2}\right)$.
        Let there be $Y$, the result of the second throw. From the given,
        $Y | \left\{X = 0\right\} \sim Ber \left(\displaystyle\frac{1}{2}\right)$,
        and $Y | \left\{X = 1\right\} \sim Ber \left(p\right)$. From the complete
        probability formula, we get that $\p \left(Y = 1\right) = \p \left(Y = 1 | X = 0\right) \cdot \p \left(X = 0\right) + \p \left(Y = 1 | X = 1\right) \cdot
        \p \left(X = 1\right) = \displaystyle\frac{1}{2} \cdot \displaystyle\frac{1}{2} + p \cdot \displaystyle\frac{1}{2} = \displaystyle\frac{1}{4} + \displaystyle\frac{p}{2}$.
        So, $Y \sim Ber \left(\displaystyle\frac{1}{4} + \displaystyle\frac{p}{2}\right)$
    \end{proof}
\end{exercise}
% section Distribution given an event (end)

\section{Independent random variables}\label{sec:Independent random variables} % (fold)
Reminder: The random variables $X, Y$ on the same probability space will be called
independent if $\forall S, T \in \F_\R$, the events $\left\{X \in S\right\}, \left\{Y \in T\right\}$
are independent. This can be written as follows:
\begin{itemize}
    \item $\p \left(\left\{X \in S\right\} \cap \left\{Y \in T\right\}\right) = \p \left(X \in S\right) \p \left(Y \in T\right)$
    \item If for all $S \in \F_\R$ such that $\p \left(X \in S\right) > 0$, then $\p_{\left(Y | \left\{X \in S\right\}\right)} = \p_Y$
\end{itemize}
If $X, Y$ are independent random variables, then the shared distribution of $Z = \left(X, Y\right)$,
is defined by the marginal distributions of $X, Y$

\begin{exercise}
    Let there be $X, Y$, two independent random variables, uniformly distributed on
    $\left\{0, 1, 2\right\}$, and we will write $Z = \left(X + Y\right) \mod 3$.
    Are $Z$ and $X$ independent?

    \begin{proof}[Solution]
        Since $Z, X$ are discrete, it is sufficient to check that for all
        $a, b \in \left\{0, 1, 2\right\}$ that $\p \left(X = a, Z = b\right) = \p \left(X = a\right) \p \left(Z = b\right)$.
        It is true that \begin{align*}
            \p \left(X = a, Z = b\right) &= \p \left(X = a, a + Y \stackrel{\mod 3}{=}  b\right) \\
                                         &= \p \left(X = a, Y \stackrel{\mod 3}{=}  b - a\right) \\
                                         &= \p \left(X = a\right) \p \left(Y \stackrel{\mod 3}{=} b - a\right)
        \end{align*}
        So the variables are independent: \[
            \p \left(Z = b\right) = \displaystyle\sum_{X \in \left\{0, 1, 2\right\}}^{}\p \left(X = x, Z = b\right) = \displaystyle\frac{1}{3}
        \]
    \end{proof}
\end{exercise}

\begin{exercise}
    $X, Y, B: \Omega \to \R$ are random variables such that there are the disjoint
    sets $S, T \subset \R$, such that
    \begin{align*}
        X &\sim Unif \left(S\right) \\
        Y &\sim Unif \left(T\right) \\
        B &\sim Unif \left(\displaystyle\frac{|S|}{|T \cup S|}\right) \\
    \end{align*}
    Show that if the pairs $\left\{B, X\right\}$ and $B, Y$ are independent,
    then $BX + \left(1 - B\right)Y \sim Unif \left(T \cup S\right)$

    \begin{proof}[Solution]
        Let there be $x \in T \cup S$, and we will write $Z = BX + \left(1 - B\right)Y$.
        So from the complete probability formula \begin{align*}
            \p \left(Z = x\right) &= \p \left(Z = x | B = 1\right) \cdot \p \left(B = 1\right) + \p \left(Z = x | B = 0\right) \p \left(B = 0\right) \\
                                  &= \p \left(X = x\right) \p \left(B = 1\right) + \p \left(Y = x\right) \p \left(B = 0\right)
        \end{align*}
        If $X \in T$: $\p \left(Y = x\right) = \displaystyle\frac{1}{|T|}$ and $\p \left(X = x\right) = 0$,
        and therefore \[
            \p \left(Z = x\right) = \p \left(Y = x\right) \p \left(B = 0\right) = \displaystyle\frac{1}{|T|} \cdot \displaystyle\frac{|T \cup S| - |S|}{|T \cup S|} = \displaystyle\frac{1}{|T \cup S|}
        \]
        If $X \in S$: $\p \left(Z = x\right) = \p \left(X = x\right) \p \left(B = 1\right) = \displaystyle\frac{1}{|S|} \displaystyle\frac{|S|}{|T \cup S|} = \displaystyle\frac{1}{|T \cup S|}$
    \end{proof}
\end{exercise}

\begin{exercise}
    Let there be a random variable $X$. Prove that $X$ is independent of itself \textbf{if and only if}
    is almost always constant.

    \begin{proof}[Solution]
        $X$ is independent of itself \textbf{if and only if} for all $A$ $\p \left(X \in A\right) = \p \left(X \in A\right)\p \left(X \in A\right) = \p \left(X \in A\right)^2$.
        Therefore $\p \left(X \in A\right) \in \left\{0, 1\right\}$. If $X$ is constant,
        then this is trivial. In the other direction, we will assume that $X$ is almost
        surely in $A$. Let there be some $B$, and so $X \in \left(A \cap B\right) \lor X \in \left(A \cap B^c\right)$. We
        will continue this process on dyadic segments, and will then conclude that $X$
        is in the nested intersection of dyadic segments, and therefore has a value
        that it almost surely takes.
    \end{proof}
\end{exercise}
% section Independent random variables (end)


\end{document}
